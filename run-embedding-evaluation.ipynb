{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b5561f0",
   "metadata": {},
   "source": [
    "# Prepare Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f42f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start evaluation...\n"
     ]
    }
   ],
   "source": [
    "# make sure that the evaluation framework directory is in PATH\n",
    "import sys\n",
    "sys.path.append('./evaluation_framework/')\n",
    "\n",
    "# initialize framework manager\n",
    "from evaluation_framework.manager import FrameworkManager\n",
    "evaluation_manager = FrameworkManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487644c5",
   "metadata": {},
   "source": [
    "# Run Evaluation Framework with Individual Embedding Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea4bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run all approaches (may take several hours per approach!)\n",
    "embedding_approaches = ['rdf2vec', 'DeepWalk', 'Node2Vec', 'KGlove', 'rdf2vecOA', 'TransE-L1', 'TransE-L2', 'TransR', 'RotatE', 'ComplEx', 'DistMult', 'RESCAL']\n",
    "for approach in embedding_approaches:\n",
    "    path_to_embedding_file = f'./data/vectors_dbpedia_{approach}.txt'\n",
    "    evaluation_manager.evaluate(path_to_embedding_file, vector_size=200, parallel=True, debugging_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5020543",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e0abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Evaluation Framework generates a file with the name `comparison.csv` containing all the results.\n",
    "# We load this file with pandas and display the relevant results here\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_colwidth', 199)\n",
    "\n",
    "# map the names of the approaches to something more readable\n",
    "datasets = {\n",
    "    'data/vectors_dbpedia_rdf2vec_200_cosine_2_1': 'Rdf2Vec',\n",
    "    'data/vectors_dbpedia_DeepWalk_200_cosine_2_1': 'DeepWalk',\n",
    "    'data/vectors_dbpedia_Node2Vec_200_cosine_2_1': 'Node2Vec',\n",
    "    'data/vectors_dbpedia_KGlove_200_cosine_2_1': 'KGlove',\n",
    "    'data/vectors_dbpedia_rdf2vecOA_200_cosine_2_1': 'Rdf2Vec_oa',\n",
    "    'data/vectors_dbpedia_TransE-L1_200_cosine_2_1': 'TransE-L1',\n",
    "    'data/vectors_dbpedia_TransE-L2_200_cosine_2_1': 'TransE-L2',\n",
    "    'data/vectors_dbpedia_TransR_200_cosine_2_1': 'TransR',\n",
    "    'data/vectors_dbpedia_RotatE_200_cosine_2_1': 'RotatE',\n",
    "    'data/vectors_dbpedia_DistMult_200_cosine_2_1': 'DistMult',\n",
    "    'data/vectors_dbpedia_RESCAL_200_cosine_2_1': 'RESCAL',\n",
    "    'data/vectors_dbpedia_ComplEx_200_cosine_2_1': 'ComplEx',\n",
    "}\n",
    "\n",
    "# load comparison.csv file\n",
    "df = pd.read_csv('./comparison.csv', sep=' ')\n",
    "\n",
    "# compute best results per gold-standard-file and select relevant metrics\n",
    "dfbest = df[(df['metric'].isin([\n",
    "    'root_mean_squared_error',  # Regression\n",
    "    'harmonic_mean',  # Document Similarity\n",
    "    'clustering_accuracy',  # Clustering\n",
    "    'kendalltau_correlation',  # Entity Relatedness\n",
    "    'accuracy'  # Classification, Semantic Analogies\n",
    "])) & (df['model'] != 'with_weights')]  # ignore weights\n",
    "pd.concat([\n",
    "    # use minimum for regression and maximum for the rest\n",
    "    dfbest[dfbest['task_name'] != 'Regression'].pivot_table(values='score_value', columns=['test_name'], index=['task_name', 'gold_standard_file', 'metric'], aggfunc='max').filter(items=list(datasets), axis='columns').rename(columns=datasets).dropna(how='all').fillna(0),\n",
    "    dfbest[dfbest['task_name'] == 'Regression'].pivot_table(values='score_value', columns=['test_name'], index=['task_name', 'gold_standard_file', 'metric'], aggfunc='min').filter(items=list(datasets), axis='columns').rename(columns=datasets).dropna(how='all').fillna(0)\n",
    "]).rename(index={'accuracy': 'ACC', 'clustering_accuracy': 'ACC', 'root_mean_squared_error': 'RMSE', 'kendalltau_correlation': 'KendallTau', 'harmonic_mean': 'HarmonicMean'}).round(decimals=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
